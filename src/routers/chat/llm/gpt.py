from .base import LLMProvider
import os
from openai import OpenAI
from typing import Generator, Tuple

class GPT4oProvider(LLMProvider):
    """
    An LLM provider that uses the OpenAI API to serve the GPT-4o model.
    """
    def __init__(self):
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            print("OPENAI_API_KEY environment variable is not set!")
            raise ValueError("OPENAI_API_KEY environment variable is required")
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4o"

    def query(self, system_prompt: str, user_prompt: str) -> Generator[str, None, None]:
        """
        Sends a streaming query to the GPT-4o model via the OpenAI API.

        Args:
            system_prompt: The instruction or context for the model's behavior.
            user_prompt: The user's direct question or input.

        Yields:
            Chunks of the text response as they are generated by the LLM.
        """
        print(f"GPT-4o query called with system prompt length: {len(system_prompt)}")
        print(f"GPT-4o query called with user prompt: {user_prompt[:100]}...")
        
        try:
            # First, make a non-streaming call to get token usage information
            completion = self.client.chat.completions.create(
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": user_prompt,
                    }
                ],
                model=self.model,
                stream=False,
            )

            # Extract and log token usage
            if completion.usage:
                input_tokens = completion.usage.prompt_tokens
                output_tokens = completion.usage.completion_tokens
                total_tokens = completion.usage.total_tokens
                print(f"üî¢ GPT-4o Token Usage - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}")
            
            # Return the response content as a generator (simulating streaming)
            response_content = completion.choices[0].message.content
            print(f"‚úÖ GPT-4o response: {response_content[:200]}...")
            if response_content:
                yield response_content
        except Exception as e:
            print(f"‚ùå GPT-4o error: {e}")
            yield "I'm sorry, I encountered an error. Please try again." 